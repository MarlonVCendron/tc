
A busca pela compreensão e reprodução das habilidades cognitivas e de aprendizado do cérebro humano tem sido um desafio constante
nas áreas de neurociência computacional e Inteligência Artificial (IA). É possível argumentar que as Redes Neurais Artificiais
(RNA) são o mais próximo que já chegamos dessa reprodução; entretanto, as RNAs deixaram de lado o realismo biológico em prol do
aperfeiçoamento da IA. As Redes Neurais de Disparos (RND)¹ representam um avanço significativo em direção ao objetivo de
compreender o cérebro humano, uma vez que buscam emular o comportamento das Redes Neurais Biológicas (RNB) de forma mais realista
do que as abordagens tradicionais.

As Redes Neurais Artificiais (RNA) convencionais são inspiradas no cérebro: neurônios disparam em determinadas frequências
conforme os sinais recebidos de conexões com outros neurônios através de sinapses plásticas, cuja força muda dinamicamente de
acordo com o treinamento. Entretanto, as semelhanças com o cérebro terminam aqui. As RNAs tradicionais não capturam a dinâmica
interna dos neurônios biológicos, que disparam de maneiras complexas e distintas, e não apenas em uma frequência constante. A
principal característica que torna as RNAs capazes de aprender é seu método de retropropagação de erro, um método de treinamento
que até pode existir em alguns casos no cérebro \cite{lillicrapBackpropagationBrain2020} \cite{songCanBrainBackpropagation2020},
mas que é diferente da forma de de aprendizado local por plasticidade das RNBs \cite{yamazakiSpikingNeuralNetworks2022}.

As RNDs são modelos muito mais próximos às RNBs que se comunicam por meio de impulsos elétricos discretos, chamados de disparos, e
que aprendem por métodos realistas, como a plasticidade nas sinapses. O grau de realismo biológico de uma RND depende de sua
implementação, podendo empregar modelos de neurônios tão simples quanto a equação única do modelo LIF, ou até modelos que simulam
canais de íons, ramificações de dendritos, entre outros. As RNDs não só representam uma possível evolução das RNAs, como também
são usadas para seu propósito original: compreender o cérebro através da simulação.

No entanto, treinar RNDs continua sendo uma tarefa desafiadora, já que os algoritmos de aprendizado empregados nas RNAs
tradicionais, além de não serem biologicamente realistas, também não são diretamente aplicáveis às SNNs devido à natureza discreta
dos disparos, que as torna não diferenciáveis, impedindo o cálculo de gradientes.

O principal método empregado para o aprendizado de RNDs é a plasticidade das sinapses. A plasticidade é a capacidade do cérebro de
se adaptar e reorganizar suas conexões neurais em resposta a novas informações, experiências ou estímulos; é a principal
propriedade por trás do aprendizado e da formação de memórias. Uma das primeiras formas de plasticidade neural descritas é a
plasticidade hebbiana, que pode resultar no fortalecimento ou enfraquecimento das sinapses com base na ativação simultânea de
neurônios conectados: caso o neurônio pós-sináptico dispare logo após o neurônio pré-sináptico, significa que há uma correlação
entre eles e a sinapse é fortalecida, caso contrário, a sinapse é enfraquecida.

A plasticidade hebbiana, porém, não descreve toda a gama de diferentes modos com que a plasticidade se manifesta no cérebro, como
é o caso das  plasticidades heterossináptica, em que a ativação de neurônios causa mudanças em neurônios inativos, e homeostática,
um processo lento em que as sinapses se auto-regulam para garantir estabilidade. A plasticidade também
depende do tipo de neurônio, do tipo da conexão, do tempo de efeito das alterações (curto ou longo-prazo), entre outros fatores. A
natureza do efeito da plasticidade também varia muito, podendo depender da frequência de disparos, da diferença de potencial, do
tempo dos disparos (como é o caso da plasticidade hebbiana), entre outros. Nas RNDs, assim como ocorre com os modelos de
neurônios, os modelos de plasticidade também possuem uma ampla variação em termos de plausibilidade biológica. Além disso,
dependendo da implementação, pode-se utilizar múltiplos modelos de plasticidade simultaneamente.

A plasticidade hebbiana pode ser sumarizada pela seguinte frase "Neurônios que disparam juntos, conectam-se juntos", assim, a
plasticidade dá origem a um fenômeno emergente no cérebro chamado de conjuntos celulares, em que grupos de neurônios relacionados
a um mesmo estímulo ou processo acabam fortalecendo as conexões entre si. Um conjunto celular pode servir como uma forma de
memória associativa \cite{sakuraiMultipleApproachesInvestigation2018}, em que a ativação de um dos neurônios do conjunto acaba por
ativar sincronamemente os demais neurônios do conjunto. Tomando como exemplo a memória de uma viagem à praia na infância: essa
memória consiste em vários elementos, como o som das ondas, a sensação de areia quente, o cheiro de água salgada e a visão de
pessoas, guarda-sóis etc. Cada um desses elementos sensoriais é processado em diferentes áreas do cérebro e ativa diferentes
grupos de neurônios. A ativação síncrona dos neurônios responsáveis por esses elementos sensoriais leva à formação de um conjunto
celular. Um tempo depois, ao sentir o cheiro do mar novamente, esse estímulo pode acabar ativando o conjunto celular, resultando
na experiência da memória.

Um conjunto celular não é uma estrutura estática, mas sim uma rede dinâmica de neurônios que podem ser modificados e reativados
ao longo do tempo.


# Motivação

# Método

// Sono

Neste contexto, o problema abordado neste trabalho consiste em explorar a retenção de memórias em uma RND. A principal hipótese a
ser avaliada neste trabalho é de que abordagens baseadas em simulações de fases do sono podem melhorar a estabilidade de conjuntos
celulares contribuindo para o processo de retenção de memórias.

# objetivos geral e específicos

Notas rodapé:

¹ Do inglês Spiking Neural Networks.


1 Introdução

texto

1.1 Objetivo geral

1.2 Objetivos específicos

1.3 Estrutura do trabalho

o trabalho se organiza assim, no capítulo 2....
