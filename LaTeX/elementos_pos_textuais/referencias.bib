@article{abbottLapicque1999,
  title = {Lapicque's Introduction of the Integrate-and-Fire Model Neuron (1907)},
  author = {Abbott, L.F},
  year = {1999},
  month = nov,
  journal = {Brain Research Bulletin},
  volume = {50},
  number = {5-6},
  pages = {303--304},
  issn = {03619230},
  doi = {10.1016/S0361-9230(99)00161-6},
  urldate = {2023-05-19},
  langid = {english},
  keywords = {read},
  annotation = {âœ…}
}

@article{aserinskyRegularly1953,
  title = {Regularly {{Occurring Periods}} of {{Eye Motility}}, and {{Concomitant Phenomena}}, {{During Sleep}}},
  author = {Aserinsky, Eugene and Kleitman, Nathaniel},
  year = {1953},
  month = sep,
  journal = {Science},
  volume = {118},
  number = {3062},
  pages = {273--274},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.118.3062.273},
  urldate = {2023-04-29},
  langid = {english},
  annotation = {ðŸš«}
}

@article{blissittSleep2001,
  title = {Sleep, {{Memory}}, and {{Learning}}:},
  shorttitle = {Sleep, {{Memory}}, and {{Learning}}},
  author = {Blissitt, Patricia A.},
  year = {2001},
  month = aug,
  journal = {Journal of Neuroscience Nursing},
  volume = {33},
  number = {4},
  pages = {208--215},
  issn = {0888-0395},
  doi = {10.1097/01376517-200108000-00007},
  urldate = {2023-04-29},
  langid = {english},
  annotation = {ðŸ“˜}
}

@article{blissLonglasting1973,
  title = {Long-Lasting Potentiation of Synaptic Transmission in the Dentate Area of the Anaesthetized Rabbit Following Stimulation of the Perforant Path},
  author = {Bliss, T. V. P. and L{\o}mo, T.},
  year = {1973},
  month = jul,
  journal = {The Journal of Physiology},
  volume = {232},
  number = {2},
  pages = {331--356},
  issn = {00223751},
  doi = {10.1113/jphysiol.1973.sp010273},
  urldate = {2023-05-20},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/FPMTWWPV/Bliss and LÃ¸mo - 1973 - Long-lasting potentiation of synaptic transmission.pdf}
}

@article{boyceREM2017,
  title = {{{REM}} Sleep and Memory},
  author = {Boyce, Richard and Williams, Sylvain and Adamantidis, Antoine},
  year = {2017},
  month = jun,
  journal = {Current Opinion in Neurobiology},
  volume = {44},
  pages = {167--177},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.05.001},
  urldate = {2023-04-30},
  langid = {english},
  keywords = {maybe},
  annotation = {ðŸ“ƒ}
}

@article{burkittReview2006,
  title = {A Review of the Integrate-and-Fire Neuron Model: {{II}}. {{Inhomogeneous}} Synaptic Input and Network Properties},
  shorttitle = {A Review of the Integrate-and-Fire Neuron Model},
  author = {Burkitt, A. N.},
  year = {2006},
  month = aug,
  journal = {Biological Cybernetics},
  volume = {95},
  number = {2},
  pages = {97--112},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-006-0082-8},
  urldate = {2023-04-17},
  langid = {english},
  annotation = {ðŸ“ƒ}
}

@article{coleELECTRIC1939,
  title = {{{ELECTRIC IMPEDANCE OF THE SQUID GIANT AXON DURING ACTIVITY}}},
  author = {Cole, Kenneth S. and Curtis, Howard J.},
  year = {1939},
  month = may,
  journal = {Journal of General Physiology},
  volume = {22},
  number = {5},
  pages = {649--670},
  issn = {1540-7748, 0022-1295},
  doi = {10.1085/jgp.22.5.649},
  urldate = {2023-05-11},
  abstract = {Alternating current impedance measurements have been made over a wide frequency range on the giant axon from the stellar nerve of the squid, Loligo pealii, during the passage of a nerve impulse. The transverse impedance was measured between narrow electrodes on either side of the axon with a Wheatstone bridge having an amplifier and cathode ray oscillograph for detector. When the bridge was balanced, the resting axon gave a narrow line on the oscillograph screen as a sweep circuit moved the spot across. As an impulse passed between impedance electrodes after the axon had been stimulated at one end, the oscillograph line first broadened into a band, indicating a bridge unbalance, and then narrowed down to balance during recovery. From measurements made during the passage of the impulse and appropriate analysis, it was found that the membrane phase angle was unchanged, the membrane capacity decreased about 2 per cent, while the membrane conductance fell from a resting value of 1000 ohm cm.2 to an average of 25 ohm cm.2             The onset of the resistance change occurs somewhat after the start of the monophasic action potential, but coincides quite closely with the point of inflection on the rising phase, where the membrane current reverses in direction, corresponding to a decrease in the membrane electromotive force. This E.M.F. and the conductance are closely associated properties of the membrane, and their sudden changes constitute, or are due to, the activity which is responsible for the all-or-none law and the initiation and propagation of the nerve impulse. These results correspond to those previously found for Nitella and lead us to expect similar phenomena in other nerve fibers.},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/BRQF4R8M/Cole and Curtis - 1939 - ELECTRIC IMPEDANCE OF THE SQUID GIANT AXON DURING .pdf}
}

@article{crepelRegression1982,
  title = {Regression of Functional Synapses in the Immature Mammalian Cerebellum},
  author = {Crepel, Francis},
  year = {1982},
  month = jan,
  journal = {Trends in Neurosciences},
  volume = {5},
  pages = {266--269},
  issn = {01662236},
  doi = {10.1016/0166-2236(82)90168-0},
  urldate = {2023-04-15},
  langid = {english},
  annotation = {ðŸš«}
}

@article{diekelmannMemory2010,
  title = {The Memory Function of Sleep},
  author = {Diekelmann, Susanne and Born, Jan},
  year = {2010},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {2},
  pages = {114--126},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2762},
  urldate = {2023-04-29},
  langid = {english},
  keywords = {to-read},
  annotation = {ðŸ“ƒ}
}

@inproceedings{dihuDigitalImplementationSpiking2014,
  title = {Digital Implementation of a Spiking Neural Network ({{SNN}}) Capable of Spike-Timing-Dependent Plasticity ({{STDP}}) Learning},
  booktitle = {14th {{IEEE International Conference}} on {{Nanotechnology}}},
  author = {{Di Hu} and {Xu Zhang} and {Ziye Xu} and Ferrari, Silvia and Mazumder, Pinaki},
  year = {2014},
  month = aug,
  pages = {873--876},
  publisher = {{IEEE}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1109/NANO.2014.6968000},
  urldate = {2023-04-01},
  isbn = {978-1-4799-5622-7},
  keywords = {maybe}
}

@article{dudekHomosynaptic1992,
  title = {Homosynaptic Long-Term Depression in Area {{CA1}} of Hippocampus and Effects of {{N-methyl-D-aspartate}} Receptor Blockade.},
  author = {Dudek, S M and Bear, M F},
  year = {1992},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {89},
  number = {10},
  pages = {4363--4367},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.89.10.4363},
  urldate = {2023-05-21},
  abstract = {We tested a theoretical prediction that patterns of excitatory input activity that consistently fail to activate target neurons sufficiently to induce synaptic potentiation will instead cause a specific synaptic depression. To realize this situation experimentally, the Schaffer collateral projection to area CA1 in rat hippocampal slices was stimulated electrically at frequencies ranging from 0.5 to 50 Hz. Nine hundred pulses at 1-3 Hz consistently yielded a depression of the CA1 population excitatory postsynaptic potential that persisted without signs of recovery for greater than 1 hr after cessation of the conditioning stimulation. This long-term depression was specific to the conditioned input, ruling out generalized changes in postsynaptic responsiveness or excitability. Three lines of evidence suggest that this effect is accounted for by a modification of synaptic effectiveness rather than damage to or fatigue of the stimulated inputs. First, the effect was dependent on the stimulation frequency; 900 pulses at 10 Hz caused no lasting change, and at 50 Hz a synaptic potentiation was usually observed. Second, the depressed synapses continued to support long-term potentiation in response to a high-frequency tetanus. Third, the effects of conditioning stimulation could be prevented by application of NMDA receptor antagonists. Thus, our data suggest that synaptic depression can be triggered by prolonged NMDA receptor activation that is below the threshold for inducing synaptic potentiation. We propose that this mechanism is important for the modifications of hippocampal response properties that underlie some forms of learning and memory.},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/D6D3SNLY/Dudek and Bear - 1992 - Homosynaptic long-term depression in area CA1 of h.pdf}
}

@article{fattSpontaneous1952,
  title = {Spontaneous Subthreshold Activity at Motor Nerve Endings},
  author = {Fatt, P. and Katz, B.},
  year = {1952},
  month = may,
  journal = {The Journal of Physiology},
  volume = {117},
  number = {1},
  pages = {109--128},
  issn = {0022-3751},
  langid = {english},
  keywords = {Humans,MYONEURAL JUNCTION,Nerve Endings,Neuromuscular Junction},
  annotation = {ðŸš«}
}

@article{feldmanSynaptic2009,
  title = {Synaptic {{Mechanisms}} for {{Plasticity}} in {{Neocortex}}},
  author = {Feldman, Daniel E.},
  year = {2009},
  month = jun,
  journal = {Annual Review of Neuroscience},
  volume = {32},
  number = {1},
  pages = {33--55},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.051508.135516},
  urldate = {2023-05-21},
  abstract = {Sensory experience and learning alter sensory representations in cerebral cortex. The synaptic mechanisms underlying sensory cortical plasticity have long been sought. Recent work indicates that long-term cortical plasticity is a complex, multicomponent process involving multiple synaptic and cellular mechanisms. Sensory use, disuse, and training drive long-term potentiation and depression (LTP and LTD), homeostatic synaptic plasticity and plasticity of intrinsic excitability, and structural changes including formation, removal, and morphological remodeling of cortical synapses and dendritic spines. Both excitatory and inhibitory circuits are strongly regulated by experience. This review summarizes these findings and proposes that these mechanisms map onto specific functional components of plasticity, which occur in common across the primary somatosensory, visual, and auditory cortices.},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/MRDSG7V5/Feldman - 2009 - Synaptic Mechanisms for Plasticity in Neocortex.pdf}
}

@article{gerstnerAssociative1992,
  title = {Associative Memory in a Network of `Spiking' Neurons},
  author = {Gerstner, Wulfram and {van Hemmen}, J Leo},
  year = {1992},
  month = jan,
  journal = {Network: Computation in Neural Systems},
  volume = {3},
  number = {2},
  pages = {139--164},
  issn = {0954-898X, 1361-6536},
  doi = {10.1088/0954-898X_3_2_004},
  urldate = {2023-04-18},
  langid = {english},
  keywords = {maybe},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/IGVD6H6K/Gerstner and van Hemmen - 1992 - Associative memory in a network of â€˜spikingâ€™ neuro.pdf}
}

@article{gerstnerHow2009,
  title = {How {{Good Are Neuron Models}}?},
  author = {Gerstner, Wulfram and Naud, Richard},
  year = {2009},
  month = oct,
  journal = {Science},
  volume = {326},
  number = {5951},
  pages = {379--380},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1181936},
  urldate = {2023-04-19},
  abstract = {A recent competition encouraged modelers to predict neuronal activity. Which neuron model performed the best?           ,                             Opinions strongly diverge on what constitutes a good model of a neuron (                                1                              \textendash{}                                3                              ). Two lines of thought on this have coexisted for a long time: detailed biophysical models (of the style proposed in 1952 by the physiologists Alan Hodgkin and Andrew Huxley) that describe ion channels on the tree-like spatial structure of the neuronal cell (                                4                              ), and simple ``integrate-and-fire'' models based on the much older insight that pulsatile electrical activity (known as an action potential or spike) is a threshold process. Electrophysiologists generally prefer the biophysical models, familiar with the notion of ion channels that open and close (and hence, alter neuronal activity) depending on environmental conditions. Theoreticians, by contrast, typically prefer simple neuron models with few parameters that are amenable to mathematical analysis. Earlier this year, following previous attempts at model comparison on a smaller scale (                                5                              ), the International Neuroinformatics Coordinating Facility (INCF) launched an international competition (                                6                              ) that allowed a quantitative comparison of neuron models.},
  langid = {english},
  keywords = {read},
  annotation = {âœ…},
  file = {/home/marlon/Zotero/storage/S2STW9KK/Gerstner and Naud - 2009 - How Good Are Neuron Models.pdf}
}

@book{gerstnerSpiking2002,
  title = {Spiking {{Neuron Models}}: {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  shorttitle = {Spiking {{Neuron Models}}},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  year = {2002},
  month = aug,
  edition = {First},
  publisher = {{Cambridge University Press}},
  urldate = {2023-04-14},
  abstract = {Neurons in the brain communicate by short electrical pulses, the so-called action potentials or spikes. How can we understand the process of spike generation? How can we understand information transmission by neurons? What happens if thousands of neurons are coupled together in a seemingly random network? How does the network connectivity determine the activity patterns? And, vice versa, how does the spike activity influence the connectivity pattern? These questions are addressed in this 2002 introduction to spiking neurons aimed at those taking courses in computational neuroscience, theoretical biology, biophysics, or neural networks. The approach will suit students of physics, mathematics, or computer science; it will also be useful for biologists who are interested in mathematical modelling. The text is enhanced by many worked examples and illustrations. There are no mathematical prerequisites beyond what the audience would meet as undergraduates: more advanced techniques are introduced in an elementary, concrete fashion when needed.},
  isbn = {978-0-521-81384-6 978-0-521-89079-3 978-0-511-81570-6},
  annotation = {ðŸ“˜}
}

@article{goldenSleep2022,
  title = {Sleep Prevents Catastrophic Forgetting in Spiking Neural Networks by Forming a Joint Synaptic Weight Representation},
  author = {Golden, Ryan and Delanois, Jean Erik and Sanda, Pavel and Bazhenov, Maxim},
  editor = {Bush, Daniel},
  year = {2022},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {11},
  pages = {e1010628},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010628},
  urldate = {2023-04-01},
  abstract = {Artificial neural networks overwrite previously learned tasks when trained sequentially, a phenomenon known as catastrophic forgetting. In contrast, the brain learns continuously, and typically learns best when new training is interleaved with periods of sleep for memory consolidation. Here we used spiking network to study mechanisms behind catastrophic forgetting and the role of sleep in preventing it. The network could be trained to learn a complex foraging task but exhibited catastrophic forgetting when trained sequentially on different tasks. In synaptic weight space, new task training moved the synaptic weight configuration away from the manifold representing old task leading to forgetting. Interleaving new task training with periods of off-line reactivation, mimicking biological sleep, mitigated catastrophic forgetting by constraining the network synaptic weight state to the  previously learned manifold, while allowing the weight configuration to converge towards the intersection of the manifolds representing old and new tasks. The study reveals a possible strategy of synaptic weights dynamics the brain applies during sleep to prevent forgetting and optimize learning.},
  langid = {english},
  keywords = {reading},
  annotation = {ðŸ“˜},
  file = {/home/marlon/Zotero/storage/2SJ3BLFE/Golden et al. - 2022 - Sleep prevents catastrophic forgetting in spiking .pdf}
}

@article{grangerExpression2014,
  title = {Expression Mechanisms Underlying Long-Term Potentiation: A Postsynaptic View, 10 Years On},
  shorttitle = {Expression Mechanisms Underlying Long-Term Potentiation},
  author = {Granger, Adam J. and Nicoll, Roger A.},
  year = {2014},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1633},
  pages = {20130136},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2013.0136},
  urldate = {2023-05-21},
  abstract = {This review focuses on the research that has occurred over the past decade which has solidified a postsynaptic expression mechanism for long-term potentiation (LTP). However, experiments that have suggested a presynaptic component are also summarized. It is argued that the pairing of glutamate uncaging onto single spines with postsynaptic depolarization provides the final and most elegant demonstration of a postsynaptic expression mechanism for NMDA receptor-dependent LTP. The fact that the magnitude of this LTP is similar to that evoked by pairing synaptic stimulation and depolarization leaves little room for a substantial presynaptic component. Finally, recent data also require a revision in our thinking about the way AMPA receptors (AMPARs) are recruited to the postsynaptic density during LTP. This recruitment is independent of subunit type, but does require an adequate reserve pool of extrasynaptic receptors.},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/75JGJ7I8/Granger and Nicoll - 2014 - Expression mechanisms underlying long-term potenti.pdf}
}

@article{Hebb1950,
  title = {Hebb, {{D}}. {{O}}. {{The}} Organization of Behavior: {{A}} Neuropsychological Theory. {{New York}}: {{John Wiley}} and {{Sons}}, {{Inc}}., 1949. 335 p. \$4.00},
  shorttitle = {Hebb, {{D}}. {{O}}. {{The}} Organization of Behavior},
  year = {1950},
  month = dec,
  journal = {Science Education},
  volume = {34},
  number = {5},
  pages = {336--337},
  issn = {0036-8326, 1098-237X},
  doi = {10.1002/sce.37303405110},
  urldate = {2023-04-15},
  langid = {english}
}

@book{hebbOrganization1949,
  title = {The {{Organization}} of {{Behavior}}},
  author = {Hebb, D.O.},
  year = {1949},
  edition = {Zeroth},
  publisher = {{Wiley \& Sons New York}},
  urldate = {2023-04-15},
  langid = {english},
  annotation = {ðŸ“˜}
}

@article{hodgkinAction1939,
  title = {Action {{Potentials Recorded}} from {{Inside}} a {{Nerve Fibre}}},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = {1939},
  month = oct,
  journal = {Nature},
  volume = {144},
  number = {3651},
  pages = {710--711},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/144710a0},
  urldate = {2023-05-11},
  langid = {english},
  annotation = {ðŸš«}
}

@article{hodgkinQuantitative1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = {1952},
  month = aug,
  journal = {The Journal of Physiology},
  volume = {117},
  number = {4},
  pages = {500--544},
  issn = {0022-3751, 1469-7793},
  doi = {10.1113/jphysiol.1952.sp004764},
  urldate = {2023-04-16},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/J4GXKUWN/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf}
}

@article{izhikevichSimple2003,
  title = {Simple Model of Spiking Neurons},
  author = {Izhikevich, E.M.},
  year = {2003},
  month = nov,
  journal = {IEEE Transactions on Neural Networks},
  volume = {14},
  number = {6},
  pages = {1569--1572},
  issn = {1045-9227},
  doi = {10.1109/TNN.2003.820440},
  urldate = {2023-04-01},
  langid = {english},
  keywords = {read},
  annotation = {âœ…},
  file = {/home/marlon/Zotero/storage/9WIDFHAL/Izhikevich - 2003 - Simple model of spiking neurons.pdf}
}

@article{jenkinsObliviscence1924,
  title = {Obliviscence during {{Sleep}} and {{Waking}}},
  author = {Jenkins, John G. and Dallenbach, Karl M.},
  year = {1924},
  month = oct,
  journal = {The American Journal of Psychology},
  volume = {35},
  number = {4},
  eprint = {1414040},
  eprinttype = {jstor},
  pages = {605},
  issn = {00029556},
  doi = {10.2307/1414040},
  urldate = {2023-04-29},
  annotation = {ðŸš«}
}

@article{jungRoastingCryogenicGrinding2020,
  title = {Roasting and {{Cryogenic Grinding Enhance}} the {{Antioxidant Property}} of {{Sword Beans}} ({{Canavalia}} Gladiata)},
  author = {Jung, Ju-Yeong and Rhee, Jin-Kyu},
  year = {2020},
  month = nov,
  journal = {Journal of Microbiology and Biotechnology},
  volume = {30},
  number = {11},
  pages = {1706--1719},
  issn = {1738-8872},
  doi = {10.4014/jmb.2003.03069},
  abstract = {The objective of this study was to optimize the conditions for enhancing the antioxidant properties of sword bean (Canavalia gladiata) as a coffee substitute in two processing methods, roasting and grinding. The optimum conditions for removing off-flavor of the bean and maximizing functionality and efficiency were light roasting and cryogenic grinding ({$<$} 53 {$\mu$}m). In these conditions, extraction yield was 16.75\%, total phenolic content (TPC) was 69.82 {$\pm$} 0.35 mg gallic acid equivalents/g, and total flavonoid content (TFC) was 168.81 {$\pm$} 1.64 mg quercetin equivalents/100 g. The antioxidant properties were 77.58 {$\pm$} 0.27\% for DPPH radical scavenging activity and 58.02 {$\pm$} 0.76 mg Trolox equivalents/g for ABTS radical scavenging activity. The values for TFC and ABTS radical scavenging activity were significantly higher (p {$<$} 0.05) than in other conditions, and TPC and DPPH radical scavenging activity were second highest in lightly roasted beans, following raw beans. HS-SPME/GCMS analysis confirmed that the amino acids and carbohydrates, which are the main components of sword bean, were condensed into other volatile flavor compounds, such as derivatives of furan, pyrazine, and pyrrole during roasting. Roasted and cryogenically ground (cryo-ground) sword beans showed higher functionality in terms of TFC, DPPH, and ABTS radical scavenging activities compared to those of coffee. Overall results showed that light roasting and cryogenic grinding are the most suitable processing conditions for enhancing the bioactivity of sword beans.},
  langid = {english},
  pmcid = {PMC9728382},
  pmid = {32830188},
  file = {/home/marlon/Zotero/storage/FBXRMWKM/Jung and Rhee - 2020 - Roasting and Cryogenic Grinding Enhance the Antiox.pdf}
}

@book{kandelPrinciples2021,
  title = {Principles of Neural Science},
  editor = {Kandel, Eric R. and Koester, John and Mack, Sarah and Siegelbaum, Steven},
  year = {2021},
  edition = {Sixth edition},
  publisher = {{McGraw Hill}},
  address = {{New York}},
  abstract = {"As in previous editions, the goal of this sixth edition of Principles of Neural Science is to provide readers with insight into how genes, molecules, neurons and the circuits they form give rise to mind"--},
  isbn = {978-1-259-64223-4},
  lccn = {QP355.2 .P76 2021},
  keywords = {Neurology,Neurons},
  annotation = {ðŸ“˜}
}

@article{lapicqueRecherches1907,
  title = {Recherches Quantitatives Sur l' Excitation Electrique Des Nerfs Traitee Comme Une Polarization.},
  author = {Lapicque, Louis},
  year = {1907},
  month = jan,
  volume = {9},
  pages = {620--635},
  annotation = {ðŸš«}
}

@article{lewisHow2018,
  title = {How {{Memory Replay}} in {{Sleep Boosts Creative Problem-Solving}}},
  author = {Lewis, Penelope A. and Knoblich, G{\"u}nther and Poe, Gina},
  year = {2018},
  month = jun,
  journal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {6},
  pages = {491--503},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.03.009},
  urldate = {2023-04-01},
  langid = {english},
  keywords = {to-read},
  annotation = {ðŸ”¥},
  file = {/home/marlon/Zotero/storage/DCMAU92E/Lewis et al. - 2018 - How Memory Replay in Sleep Boosts Creative Problem.pdf}
}

@article{lillicrapBackpropagation2020,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {21},
  number = {6},
  pages = {335--346},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0277-3},
  urldate = {2023-04-01},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/98D6X8UH/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf}
}

@article{louisQuantitative2007,
  title = {Quantitative Investigations of Electrical Nerve Excitation Treated as Polarization},
  shorttitle = {Quantitative Investigations of Electrical Nerve Excitation Treated as Polarization},
  author = {Louis, Lapicque},
  year = {2007},
  month = dec,
  journal = {Biological Cybernetics},
  volume = {97},
  number = {5-6},
  pages = {341--349},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-007-0189-6},
  urldate = {2023-04-30},
  langid = {english},
  annotation = {ðŸš«}
}

@article{maassNetworks1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  year = {1997},
  month = dec,
  journal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {08936080},
  doi = {10.1016/S0893-6080(97)00011-7},
  urldate = {2023-05-21},
  langid = {english},
  keywords = {to-read},
  annotation = {ðŸ”¥}
}

@article{malenkaLongTerm1999,
  title = {Long-{{Term Potentiation--A Decade}} of {{Progress}}?},
  author = {Malenka, Robert C. and Nicoll, {and} Roger A.},
  year = {1999},
  month = sep,
  journal = {Science},
  volume = {285},
  number = {5435},
  pages = {1870--1874},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.285.5435.1870},
  urldate = {2023-05-21},
  abstract = {Long-term potentiation of synaptic transmission in the hippocampus is the leading experimental model for the synaptic changes that may underlie learning and memory. This review presents a current understanding of the molecular mechanisms of this long-lasting increase in synaptic strength and describes a simple model that unifies much of the data that previously were viewed as contradictory.},
  langid = {english},
  annotation = {ðŸš«}
}

@article{mccullochLogical1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  journal = {The Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02478259},
  urldate = {2023-04-17},
  langid = {english},
  annotation = {ðŸš«}
}

@article{mongilloSynaptic2008,
  title = {Synaptic {{Theory}} of {{Working Memory}}},
  author = {Mongillo, Gianluigi and Barak, Omri and Tsodyks, Misha},
  year = {2008},
  month = mar,
  journal = {Science},
  volume = {319},
  number = {5869},
  pages = {1543--1546},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1150769},
  urldate = {2023-04-02},
  abstract = {It is usually assumed that enhanced spiking activity in the form of persistent reverberation for several seconds is the neural correlate of working memory. Here, we propose that working memory is sustained by calcium-mediated synaptic facilitation in the recurrent connections of neocortical networks. In this account, the presynaptic residual calcium is used as a buffer that is loaded, refreshed, and read out by spiking activity. Because of the long time constants of calcium kinetics, the refresh rate can be low, resulting in a mechanism that is metabolically efficient and robust. The duration and stability of working memory can be regulated by modulating the spontaneous activity in the network.},
  langid = {english},
  annotation = {ðŸ“ƒ}
}

@article{morrisHebb1999,
  title = {D.{{O}}. {{Hebb}}: {{The Organization}} of {{Behavior}}, {{Wiley}}: {{New York}}; 1949},
  shorttitle = {D.{{O}}. {{Hebb}}},
  author = {Morris, R.G.M},
  year = {1999},
  month = nov,
  journal = {Brain Research Bulletin},
  volume = {50},
  number = {5-6},
  pages = {437},
  issn = {03619230},
  doi = {10.1016/S0361-9230(99)00182-3},
  urldate = {2023-04-15},
  langid = {english}
}

@article{pagkalosIntroducing2023,
  title = {Introducing the {{Dendrify}} Framework for Incorporating Dendrites to Spiking Neural Networks},
  author = {Pagkalos, Michalis and Chavlis, Spyridon and Poirazi, Panayiota},
  year = {2023},
  month = jan,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {131},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-35747-8},
  urldate = {2023-04-01},
  abstract = {Abstract             Computational modeling has been indispensable for understanding how subcellular neuronal features influence circuit processing. However, the role of dendritic computations in network-level operations remains largely unexplored. This is partly because existing tools do not allow the development of realistic and efficient network models that account for dendrites. Current spiking neural networks, although efficient, are usually quite simplistic, overlooking essential dendritic properties. Conversely, circuit models with morphologically detailed neuron models are computationally costly, thus impractical for large-network simulations. To bridge the gap between these two extremes and facilitate the adoption of dendritic features in spiking neural networks, we introduce Dendrify, an open-source Python package based on Brian 2. Dendrify, through simple commands, automatically generates reduced compartmental neuron models with simplified yet biologically relevant dendritic and synaptic integrative properties. Such models strike a good balance between flexibility, performance, and biological accuracy, allowing us to explore dendritic contributions to network-level functions while paving the way for developing more powerful neuromorphic systems.},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/C6VCL2M8/Pagkalos et al. - 2023 - Introducing the Dendrify framework for incorporati.pdf}
}

@article{pagkalosIntroducing2023a,
  title = {Introducing the {{Dendrify}} Framework for Incorporating Dendrites to Spiking Neural Networks},
  author = {Pagkalos, Michalis and Chavlis, Spyridon and Poirazi, Panayiota},
  year = {2023},
  month = jan,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {131},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-35747-8},
  urldate = {2023-04-30},
  abstract = {Abstract             Computational modeling has been indispensable for understanding how subcellular neuronal features influence circuit processing. However, the role of dendritic computations in network-level operations remains largely unexplored. This is partly because existing tools do not allow the development of realistic and efficient network models that account for dendrites. Current spiking neural networks, although efficient, are usually quite simplistic, overlooking essential dendritic properties. Conversely, circuit models with morphologically detailed neuron models are computationally costly, thus impractical for large-network simulations. To bridge the gap between these two extremes and facilitate the adoption of dendritic features in spiking neural networks, we introduce Dendrify, an open-source Python package based on Brian 2. Dendrify, through simple commands, automatically generates reduced compartmental neuron models with simplified yet biologically relevant dendritic and synaptic integrative properties. Such models strike a good balance between flexibility, performance, and biological accuracy, allowing us to explore dendritic contributions to network-level functions while paving the way for developing more powerful neuromorphic systems.},
  langid = {english},
  file = {/home/marlon/Zotero/storage/RSJTB8U6/Pagkalos et al. - 2023 - Introducing the Dendrify framework for incorporati.pdf}
}

@article{peyracheMechanism2020,
  title = {A Mechanism for Learning with Sleep Spindles},
  author = {Peyrache, Adrien and Seibt, Julie},
  year = {2020},
  month = may,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1799},
  pages = {20190230},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2019.0230},
  urldate = {2023-04-01},
  abstract = {Spindles are ubiquitous oscillations during non-rapid eye movement (NREM) sleep. A growing body of evidence points to a possible link with learning and memory, and the underlying mechanisms are now starting to be unveiled. Specifically, spindles are associated with increased dendritic activity and high intracellular calcium levels, a situation favourable to plasticity, as well as with control of spiking output by feed-forward inhibition. During spindles, thalamocortical networks become unresponsive to inputs, thus potentially preventing interference between memory-related internal information processing and extrinsic signals. At the system level, spindles are co-modulated with other major NREM oscillations, including hippocampal sharp wave-ripples (SWRs) and neocortical slow waves, both previously shown to be associated with learning and memory. The sequential occurrence of reactivation at the time of SWRs followed by neuronal plasticity-promoting spindles is a possible mechanism to explain NREM sleep-dependent consolidation of memories.             This article is part of the Theo Murphy meeting issue `Memory reactivation: replaying events past, present and future'.},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/NZCUL9TN/Peyrache and Seibt - 2020 - A mechanism for learning with sleep spindles.pdf}
}

@article{ponulakIntroduction2011,
  title = {Introduction to Spiking Neural Networks: {{Information}} Processing, Learning and Applications},
  shorttitle = {Introduction to Spiking Neural Networks},
  author = {Ponulak, Filip and Kasinski, Andrzej},
  year = {2011},
  journal = {Acta Neurobiologiae Experimentalis},
  volume = {71},
  number = {4},
  pages = {409--433},
  issn = {1689-0035},
  abstract = {The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.},
  langid = {english},
  keywords = {Action Potentials,Algorithms,Animals,Humans,Learning,{Models, Neurological},Nerve Net,Neuronal Plasticity,Neurons,Synapses,Synaptic Transmission,Time Factors},
  annotation = {ðŸš«}
}

@article{sakuraiCellAssembly1998,
  title = {Cell-{{Assembly Coding}} in {{Several Memory Processes}}},
  author = {Sakurai, Yoshio},
  year = {1998},
  month = jul,
  journal = {Neurobiology of Learning and Memory},
  volume = {70},
  number = {1-2},
  pages = {212--225},
  issn = {10747427},
  doi = {10.1006/nlme.1998.3849},
  urldate = {2023-04-12},
  langid = {english},
  keywords = {to-read},
  annotation = {ðŸ”¥}
}

@article{sakuraiMultiple2018,
  title = {Multiple {{Approaches}} to the {{Investigation}} of {{Cell Assembly}} in {{Memory Research}}\textemdash{{Present}} and {{Future}}},
  author = {Sakurai, Yoshio and Osako, Yuma and Tanisumi, Yuta and Ishihara, Eriko and Hirokawa, Junya and Manabe, Hiroyuki},
  year = {2018},
  month = may,
  journal = {Frontiers in Systems Neuroscience},
  volume = {12},
  pages = {21},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2018.00021},
  urldate = {2023-04-12},
  keywords = {to-read},
  annotation = {ðŸ”¥},
  file = {/home/marlon/Zotero/storage/XHQDHH8B/Sakurai et al. - 2018 - Multiple Approaches to the Investigation of Cell A.pdf}
}

@article{santiagoCroonian1894,
  title = {The {{Croonian}} Lecture.\textemdash{{La}} Fine Structure Des Centres Nerveux},
  author = {Santiago, Ram{\'o}n y Cajal},
  year = {1894},
  month = dec,
  journal = {Proceedings of the Royal Society of London},
  volume = {55},
  number = {331-335},
  pages = {444--468},
  issn = {0370-1662, 2053-9126},
  doi = {10.1098/rspl.1894.0063},
  urldate = {2023-05-20},
  abstract = {A l'invitation gracieuse que m'ont faite les honorables membres de cette soci\'et\'e savante de venir dans cette s\'eance rendre compte de mes travaux sur la structure des centres nerveux, mon premier dessein, je ne le cacherai pas, a \'et\'e de renoncer \`a un honneur que je jugeais par trop disproportionn\'e avec mes m\'erites; mais je songeai ensuite que votre bienveillance \`a m'\'ecouter ne saurait \^etre moindre que la g\'en\'erosit\'e de votre invitation, et je me suis r\'esign\'e au r\^ole, peu flatteur du reste, d'interrompre un moment l'harmonieux concert de vos beaux travaux. J'ai d'autant plus besoin de toute vobre indulgence que je vais vous entretenir d'un sujet qui vous est parfaitement connu. Tout ce que je vais vous dire, des ma\^itres aussi \'eminents que His, K\"olliker, Waldeyer, von Lenhoss\'ek, van Gehuchten, l'ont d\'ej\`a publi\'e et r\'esum\'e d'une mani\`ere presque irr\'eprochable.},
  langid = {english},
  annotation = {ðŸš«},
  file = {/home/marlon/Zotero/storage/K7FCUFCP/1894 - The Croonian lecture.â€”La fine structure des centre.pdf}
}

@article{schliebsEvolvingSpikingNeural2013,
  title = {Evolving Spiking Neural Network\textemdash a Survey},
  author = {Schliebs, Stefan and Kasabov, Nikola},
  year = {2013},
  month = jun,
  journal = {Evolving Systems},
  volume = {4},
  number = {2},
  pages = {87--98},
  issn = {1868-6478, 1868-6486},
  doi = {10.1007/s12530-013-9074-9},
  urldate = {2023-04-01},
  langid = {english},
  keywords = {maybe},
  file = {/home/marlon/Zotero/storage/5U34DV2E/Schliebs and Kasabov - 2013 - Evolving spiking neural networkâ€”a survey.pdf}
}

@article{schulzRethinking2008,
  title = {Rethinking Sleep Analysis},
  author = {Schulz, Hartmut},
  year = {2008},
  month = apr,
  journal = {Journal of clinical sleep medicine: JCSM: official publication of the American Academy of Sleep Medicine},
  volume = {4},
  number = {2},
  pages = {99--103},
  issn = {1550-9389},
  abstract = {Visual sleep scoring is the obligatory reference for sleep analysis. An essential step in sleep scoring is sleep staging. This technique was first described in 1937 and later adapted 3 times: first, in 1957, after the detection of rapid eye movement (REM) sleep, when electrooculography (EOG) was added; second, in 1968, when sleep staging was standardized and electromyography (EMG) was added; and third, in 2007, to integrate accumulated knowledge from sleep science, adding arousals and respiratory, cardiac, and movement events. In spite of the dramatic changes that have taken place in recording and storing techniques, sleep staging has undergone surprisingly few changes. The argument of the present comment is that sleep staging was appropriate as long as sleep biosignals were recorded in the analog mode as curves on paper, whereas this staging may be insufficient for digitally recorded and stored sleep data. Limitations of sleep staging are critically discussed and alternative strategies of sleep analysis are emphasized.},
  langid = {english},
  keywords = {Electroencephalography,Electromyography,Electrooculography,Humans,Sleep Stages,Sleep Wake Disorders},
  annotation = {ðŸš«}
}

@article{songCan2020,
  title = {Can the {{Brain Do Backpropagation}}? -{{Exact Implementation}} of {{Backpropagation}} in {{Predictive Coding Networks}}},
  shorttitle = {Can the {{Brain Do Backpropagation}}?},
  author = {Song, Yuhang and Lukasiewicz, Thomas and Xu, Zhenghua and Bogacz, Rafal},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {22566--22579},
  issn = {1049-5258},
  abstract = {Backpropagation (BP) has been the most successful algorithm used to train artificial neural networks. However, there are several gaps between BP and learning in biologically plausible neuronal networks of the brain (learning in the brain, or simply BL, for short), in particular, (1) it has been unclear to date, if BP can be implemented exactly via BL, (2) there is a lack of local plasticity in BP, i.e., weight updates require information that is not locally available, while BL utilizes only locally available information, and (3) there is a lack of autonomy in BP, i.e., some external control over the neural network is required (e.g., switching between prediction and learning stages requires changes to dynamics and synaptic plasticity rules), while BL works fully autonomously. Bridging such gaps, i.e., understanding how BP can be approximated by BL, has been of major interest in both neuroscience and machine learning. Despite tremendous efforts, however, no previous model has bridged the gaps at a degree of demonstrating an equivalence to BP, instead, only approximations to BP have been shown. Here, we present for the first time a framework within BL that bridges the above crucial gaps. We propose a BL model that (1) produces exactly the same updates of the neural weights as BP, while (2) employing local plasticity, i.e., all neurons perform only local computations, done simultaneously. We then modify it to an alternative BL model that (3) also works fully autonomously. Overall, our work provides important evidence for the debate on the long-disputed question whether the brain can perform BP.},
  langid = {english},
  annotation = {ðŸ“ƒ}
}

@article{teeterGeneralized2018,
  title = {Generalized Leaky Integrate-and-Fire Models Classify Multiple Neuron Types},
  author = {Teeter, Corinne and Iyer, Ramakrishnan and Menon, Vilas and Gouwens, Nathan and Feng, David and Berg, Jim and Szafer, Aaron and Cain, Nicholas and Zeng, Hongkui and Hawrylycz, Michael and Koch, Christof and Mihalas, Stefan},
  year = {2018},
  month = feb,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {709},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-02717-4},
  urldate = {2023-04-19},
  abstract = {Abstract             There is a high diversity of neuronal types in the mammalian neocortex. To facilitate construction of system models with multiple cell types, we generate a database of point models associated with the Allen Cell Types Database. We construct a set of generalized leaky integrate-and-fire (GLIF) models of increasing complexity to reproduce the spiking behaviors of 645 recorded neurons from 16 transgenic lines. The more complex models have an increased capacity to predict spiking behavior of hold-out stimuli. We use unsupervised methods to classify cell types, and find that high level GLIF model parameters are able to differentiate transgenic lines comparable to electrophysiological features. The more complex model parameters also have an increased ability to differentiate between transgenic lines. Thus, creating simple models is an effective dimensionality reduction technique that enables the differentiation of cell types from electrophysiological responses without the need for a priori-defined features. This database will provide a set of simplified models of multiple cell types for the community to use in network models.},
  langid = {english},
  annotation = {ðŸ“˜},
  file = {/home/marlon/Zotero/storage/NN6C5MBD/Teeter et al. - 2018 - Generalized leaky integrate-and-fire models classi.pdf}
}

@article{thieleWakesleep2017,
  title = {A Wake-Sleep Algorithm for Recurrent, Spiking Neural Networks},
  author = {Thiele, Johannes and Diehl, Peter and Cook, Matthew},
  year = {2017},
  doi = {10.48550/ARXIV.1703.06290},
  urldate = {2023-04-01},
  abstract = {We investigate a recently proposed model for cortical computation which performs relational inference. It consists of several interconnected, structurally equivalent populations of leaky integrate-and-fire (LIF) neurons, which are trained in a self-organized fashion with spike-timing dependent plasticity (STDP). Despite its robust learning dynamics, the model is susceptible to a problem typical for recurrent networks which use a correlation based (Hebbian) learning rule: if trained with high learning rates, the recurrent connections can cause strong feedback loops in the network dynamics, which lead to the emergence of attractor states. This causes a strong reduction in the number of representable patterns and a decay in the inference ability of the network. As a solution, we introduce a conceptually very simple "wake-sleep" algorithm: during the wake phase, training is executed normally, while during the sleep phase, the network "dreams" samples from its generative model, which are induced by random input. This process allows us to activate the attractor states in the network, which can then be unlearned effectively by an anti-Hebbian mechanism. The algorithm allows us to increase learning rates up to a factor of ten while avoiding clustering, which allows the network to learn several times faster. Also for low learning rates, where clustering is not an issue, it improves convergence speed and reduces the final inference error.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {read},
  annotation = {âœ…}
}

@article{turrigianoHomeostatic2004,
  title = {Homeostatic Plasticity in the Developing Nervous System},
  author = {Turrigiano, Gina G. and Nelson, Sacha B.},
  year = {2004},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {5},
  number = {2},
  pages = {97--107},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1327},
  urldate = {2023-04-01},
  langid = {english},
  annotation = {ðŸ“ƒ}
}

@article{walkerSleep2006,
  title = {Sleep, {{Memory}}, and {{Plasticity}}},
  author = {Walker, Matthew P. and Stickgold, Robert},
  year = {2006},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {57},
  number = {1},
  pages = {139--166},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.56.091103.070307},
  urldate = {2023-04-29},
  abstract = {Although the functions of sleep remain largely unknown, one of the most exciting hypotheses is that sleep contributes importantly to processes of memory and brain plasticity. Over the past decade, a large body of work, spanning most of the neurosciences, has provided a substantive body of evidence supporting this role of sleep in what is becoming known as sleep-dependent memory processing. We review these findings, focusing specifically on the role of sleep in (a) memory encoding, (b) memory consolidation, (c) brain plasticity, and (d) memory reconsolidation; we finish with a summary of the field and its potential future directions.},
  langid = {english},
  annotation = {ðŸ“˜}
}

@article{yamazakiSpiking2022,
  title = {Spiking {{Neural Networks}} and {{Their Applications}}: {{A Review}}},
  shorttitle = {Spiking {{Neural Networks}} and {{Their Applications}}},
  author = {Yamazaki, Kashu and {Vo-Ho}, Viet-Khoa and Bulsara, Darshan and Le, Ngan},
  year = {2022},
  month = jun,
  journal = {Brain Sciences},
  volume = {12},
  number = {7},
  pages = {863},
  issn = {2076-3425},
  doi = {10.3390/brainsci12070863},
  urldate = {2023-04-01},
  abstract = {The past decade has witnessed the great success of deep neural networks in various domains. However, deep neural networks are very resource-intensive in terms of energy consumption, data requirements, and high computational costs. With the recent increasing need for the autonomy of machines in the real world, e.g., self-driving vehicles, drones, and collaborative robots, exploitation of deep neural networks in those applications has been actively investigated. In those applications, energy and computational efficiencies are especially important because of the need for real-time responses and the limited energy supply. A promising solution to these previously infeasible applications has recently been given by biologically plausible spiking neural networks. Spiking neural networks aim to bridge the gap between neuroscience and machine learning, using biologically realistic models of neurons to carry out the computation. Due to their functional similarity to the biological neural network, spiking neural networks can embrace the sparsity found in biology and are highly compatible with temporal code. Our contributions in this work are: (i) we give a comprehensive review of theories of biological neurons; (ii) we present various existing spike-based neuron models, which have been studied in neuroscience; (iii) we detail synapse models; (iv) we provide a review of artificial neural networks; (v) we provide detailed guidance on how to train spike-based neuron models; (vi) we revise available spike-based neuron frameworks that have been developed to support implementing spiking neural networks; (vii) finally, we cover existing spiking neural network applications in computer vision and robotics domains. The paper concludes with discussions of future perspectives.},
  langid = {english},
  keywords = {read},
  annotation = {âœ…},
  file = {/home/marlon/Zotero/storage/MF79HDU6/Yamazaki et al. - 2022 - Spiking Neural Networks and Their Applications A .pdf}
}

@article{zenkeDiverse2015,
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  year = {2015},
  month = apr,
  journal = {Nature Communications},
  volume = {6},
  number = {1},
  pages = {6922},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  urldate = {2023-04-01},
  abstract = {Abstract             Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.},
  langid = {english},
  keywords = {reading},
  annotation = {ðŸ“˜},
  file = {/home/marlon/Zotero/storage/RN27HNXU/Zenke et al. - 2015 - Diverse synaptic plasticity mechanisms orchestrate.pdf}
}

@article{zenkeDiverseSynapticPlasticity2015a,
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  year = {2015},
  month = apr,
  journal = {Nature Communications},
  volume = {6},
  number = {1},
  pages = {6922},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  urldate = {2023-04-01},
  abstract = {Abstract             Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.},
  langid = {english},
  file = {/home/marlon/Zotero/storage/CXDTBB3D/Zenke et al. - 2015 - Diverse synaptic plasticity mechanisms orchestrate.pdf}
}

@article{zenkeTemporal2017,
  title = {The Temporal Paradox of {{Hebbian}} Learning and Homeostatic Plasticity},
  author = {Zenke, Friedemann and Gerstner, Wulfram and Ganguli, Surya},
  year = {2017},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {43},
  pages = {166--176},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.03.015},
  urldate = {2023-04-01},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/QZDW6FPV/Zenke et al. - 2017 - The temporal paradox of Hebbian learning and homeo.pdf}
}

@article{zuckerShortTerm2002,
  title = {Short-{{Term Synaptic Plasticity}}},
  author = {Zucker, Robert S. and Regehr, Wade G.},
  year = {2002},
  month = mar,
  journal = {Annual Review of Physiology},
  volume = {64},
  number = {1},
  pages = {355--405},
  issn = {0066-4278, 1545-1585},
  doi = {10.1146/annurev.physiol.64.092501.114547},
  urldate = {2023-05-21},
  abstract = {{$\blacksquare$} Abstract\hspace{0.6em} Synaptic transmission is a dynamic process. Postsynaptic responses wax and wane as presynaptic activity evolves. This prominent characteristic of chemical synaptic transmission is a crucial determinant of the response properties of synapses and, in turn, of the stimulus properties selected by neural networks and of the patterns of activity generated by those networks. This review focuses on synaptic changes that result from prior activity in the synapse under study, and is restricted to short-term effects that last for at most a few minutes. Forms of synaptic enhancement, such as facilitation, augmentation, and post-tetanic potentiation, are usually attributed to effects of a residual elevation in presynaptic [Ca               2+               ]               i               , acting on one or more molecular targets that appear to be distinct from the secretory trigger responsible for fast exocytosis and phasic release of transmitter to single action potentials. We discuss the evidence for this hypothesis, and the origins of the different kinetic phases of synaptic enhancement, as well as the interpretation of statistical changes in transmitter release and roles played by other factors such as alterations in presynaptic Ca               2+               influx or postsynaptic levels of [Ca               2+               ]               i               . Synaptic depression dominates enhancement at many synapses. Depression is usually attributed to depletion of some pool of readily releasable vesicles, and various forms of the depletion model are discussed. Depression can also arise from feedback activation of presynaptic receptors and from postsynaptic processes such as receptor desensitization. In addition, glial-neuronal interactions can contribute to short-term synaptic plasticity. Finally, we summarize the recent literature on putative molecular players in synaptic plasticity and the effects of genetic manipulations and other modulatory influences.},
  langid = {english},
  annotation = {ðŸš«}
}
