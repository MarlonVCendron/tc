@inproceedings{dihuDigitalImplementationSpiking2014,
  title = {Digital Implementation of a Spiking Neural Network ({{SNN}}) Capable of Spike-Timing-Dependent Plasticity ({{STDP}}) Learning},
  booktitle = {14th {{IEEE International Conference}} on {{Nanotechnology}}},
  author = {{Di Hu} and {Xu Zhang} and {Ziye Xu} and Ferrari, Silvia and Mazumder, Pinaki},
  year = {2014},
  month = aug,
  pages = {873--876},
  publisher = {{IEEE}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1109/NANO.2014.6968000},
  urldate = {2023-04-01},
  isbn = {978-1-4799-5622-7},
  keywords = {maybe}
}

@article{goldenSleepPreventsCatastrophic2022,
  title = {Sleep Prevents Catastrophic Forgetting in Spiking Neural Networks by Forming a Joint Synaptic Weight Representation},
  author = {Golden, Ryan and Delanois, Jean Erik and Sanda, Pavel and Bazhenov, Maxim},
  editor = {Bush, Daniel},
  year = {2022},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {11},
  pages = {e1010628},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010628},
  urldate = {2023-04-01},
  abstract = {Artificial neural networks overwrite previously learned tasks when trained sequentially, a phenomenon known as catastrophic forgetting. In contrast, the brain learns continuously, and typically learns best when new training is interleaved with periods of sleep for memory consolidation. Here we used spiking network to study mechanisms behind catastrophic forgetting and the role of sleep in preventing it. The network could be trained to learn a complex foraging task but exhibited catastrophic forgetting when trained sequentially on different tasks. In synaptic weight space, new task training moved the synaptic weight configuration away from the manifold representing old task leading to forgetting. Interleaving new task training with periods of off-line reactivation, mimicking biological sleep, mitigated catastrophic forgetting by constraining the network synaptic weight state to the  previously learned manifold, while allowing the weight configuration to converge towards the intersection of the manifolds representing old and new tasks. The study reveals a possible strategy of synaptic weights dynamics the brain applies during sleep to prevent forgetting and optimize learning.},
  langid = {english},
  keywords = {reading},
  annotation = {ðŸ“˜},
  file = {/home/marlon/Zotero/storage/2SJ3BLFE/Golden et al. - 2022 - Sleep prevents catastrophic forgetting in spiking .pdf}
}

@article{izhikevichSimpleModelSpiking2003,
  title = {Simple Model of Spiking Neurons},
  author = {Izhikevich, E.M.},
  year = {2003},
  month = nov,
  journal = {IEEE Transactions on Neural Networks},
  volume = {14},
  number = {6},
  pages = {1569--1572},
  issn = {1045-9227},
  doi = {10.1109/TNN.2003.820440},
  urldate = {2023-04-01},
  langid = {english},
  keywords = {read},
  annotation = {âœ…},
  file = {/home/marlon/Zotero/storage/9WIDFHAL/Izhikevich - 2003 - Simple model of spiking neurons.pdf}
}

@article{jungRoastingCryogenicGrinding2020,
  title = {Roasting and {{Cryogenic Grinding Enhance}} the {{Antioxidant Property}} of {{Sword Beans}} ({{Canavalia}} Gladiata)},
  author = {Jung, Ju-Yeong and Rhee, Jin-Kyu},
  year = {2020},
  month = nov,
  journal = {Journal of Microbiology and Biotechnology},
  volume = {30},
  number = {11},
  pages = {1706--1719},
  issn = {1738-8872},
  doi = {10.4014/jmb.2003.03069},
  abstract = {The objective of this study was to optimize the conditions for enhancing the antioxidant properties of sword bean (Canavalia gladiata) as a coffee substitute in two processing methods, roasting and grinding. The optimum conditions for removing off-flavor of the bean and maximizing functionality and efficiency were light roasting and cryogenic grinding ({$<$} 53 {$\mu$}m). In these conditions, extraction yield was 16.75\%, total phenolic content (TPC) was 69.82 {$\pm$} 0.35 mg gallic acid equivalents/g, and total flavonoid content (TFC) was 168.81 {$\pm$} 1.64 mg quercetin equivalents/100 g. The antioxidant properties were 77.58 {$\pm$} 0.27\% for DPPH radical scavenging activity and 58.02 {$\pm$} 0.76 mg Trolox equivalents/g for ABTS radical scavenging activity. The values for TFC and ABTS radical scavenging activity were significantly higher (p {$<$} 0.05) than in other conditions, and TPC and DPPH radical scavenging activity were second highest in lightly roasted beans, following raw beans. HS-SPME/GCMS analysis confirmed that the amino acids and carbohydrates, which are the main components of sword bean, were condensed into other volatile flavor compounds, such as derivatives of furan, pyrazine, and pyrrole during roasting. Roasted and cryogenically ground (cryo-ground) sword beans showed higher functionality in terms of TFC, DPPH, and ABTS radical scavenging activities compared to those of coffee. Overall results showed that light roasting and cryogenic grinding are the most suitable processing conditions for enhancing the bioactivity of sword beans.},
  langid = {english},
  pmcid = {PMC9728382},
  pmid = {32830188},
  file = {/home/marlon/Zotero/storage/FBXRMWKM/Jung and Rhee - 2020 - Roasting and Cryogenic Grinding Enhance the Antiox.pdf}
}

@article{lewisHowMemoryReplay2018,
  title = {How {{Memory Replay}} in {{Sleep Boosts Creative Problem-Solving}}},
  author = {Lewis, Penelope A. and Knoblich, G{\"u}nther and Poe, Gina},
  year = {2018},
  month = jun,
  journal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {6},
  pages = {491--503},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.03.009},
  urldate = {2023-04-01},
  langid = {english},
  keywords = {to-read},
  annotation = {ðŸ”¥},
  file = {/home/marlon/Zotero/storage/DCMAU92E/Lewis et al. - 2018 - How Memory Replay in Sleep Boosts Creative Problem.pdf}
}

@article{lillicrapBackpropagationBrain2020,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {21},
  number = {6},
  pages = {335--346},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0277-3},
  urldate = {2023-04-01},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/98D6X8UH/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf}
}

@article{pagkalosIntroducingDendrifyFramework2023,
  title = {Introducing the {{Dendrify}} Framework for Incorporating Dendrites to Spiking Neural Networks},
  author = {Pagkalos, Michalis and Chavlis, Spyridon and Poirazi, Panayiota},
  year = {2023},
  month = jan,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {131},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-35747-8},
  urldate = {2023-04-01},
  abstract = {Abstract             Computational modeling has been indispensable for understanding how subcellular neuronal features influence circuit processing. However, the role of dendritic computations in network-level operations remains largely unexplored. This is partly because existing tools do not allow the development of realistic and efficient network models that account for dendrites. Current spiking neural networks, although efficient, are usually quite simplistic, overlooking essential dendritic properties. Conversely, circuit models with morphologically detailed neuron models are computationally costly, thus impractical for large-network simulations. To bridge the gap between these two extremes and facilitate the adoption of dendritic features in spiking neural networks, we introduce Dendrify, an open-source Python package based on Brian 2. Dendrify, through simple commands, automatically generates reduced compartmental neuron models with simplified yet biologically relevant dendritic and synaptic integrative properties. Such models strike a good balance between flexibility, performance, and biological accuracy, allowing us to explore dendritic contributions to network-level functions while paving the way for developing more powerful neuromorphic systems.},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/C6VCL2M8/Pagkalos et al. - 2023 - Introducing the Dendrify framework for incorporati.pdf}
}

@article{peyracheMechanismLearningSleep2020,
  title = {A Mechanism for Learning with Sleep Spindles},
  author = {Peyrache, Adrien and Seibt, Julie},
  year = {2020},
  month = may,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1799},
  pages = {20190230},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2019.0230},
  urldate = {2023-04-01},
  abstract = {Spindles are ubiquitous oscillations during non-rapid eye movement (NREM) sleep. A growing body of evidence points to a possible link with learning and memory, and the underlying mechanisms are now starting to be unveiled. Specifically, spindles are associated with increased dendritic activity and high intracellular calcium levels, a situation favourable to plasticity, as well as with control of spiking output by feed-forward inhibition. During spindles, thalamocortical networks become unresponsive to inputs, thus potentially preventing interference between memory-related internal information processing and extrinsic signals. At the system level, spindles are co-modulated with other major NREM oscillations, including hippocampal sharp wave-ripples (SWRs) and neocortical slow waves, both previously shown to be associated with learning and memory. The sequential occurrence of reactivation at the time of SWRs followed by neuronal plasticity-promoting spindles is a possible mechanism to explain NREM sleep-dependent consolidation of memories.             This article is part of the Theo Murphy meeting issue `Memory reactivation: replaying events past, present and future'.},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/NZCUL9TN/Peyrache and Seibt - 2020 - A mechanism for learning with sleep spindles.pdf}
}

@article{schliebsEvolvingSpikingNeural2013,
  title = {Evolving Spiking Neural Network\textemdash a Survey},
  author = {Schliebs, Stefan and Kasabov, Nikola},
  year = {2013},
  month = jun,
  journal = {Evolving Systems},
  volume = {4},
  number = {2},
  pages = {87--98},
  issn = {1868-6478, 1868-6486},
  doi = {10.1007/s12530-013-9074-9},
  urldate = {2023-04-01},
  langid = {english},
  keywords = {maybe},
  file = {/home/marlon/Zotero/storage/5U34DV2E/Schliebs and Kasabov - 2013 - Evolving spiking neural networkâ€”a survey.pdf}
}

@article{songCanBrainBackpropagation2020,
  title = {Can the {{Brain Do Backpropagation}}? -{{Exact Implementation}} of {{Backpropagation}} in {{Predictive Coding Networks}}},
  shorttitle = {Can the {{Brain Do Backpropagation}}?},
  author = {Song, Yuhang and Lukasiewicz, Thomas and Xu, Zhenghua and Bogacz, Rafal},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {22566--22579},
  issn = {1049-5258},
  abstract = {Backpropagation (BP) has been the most successful algorithm used to train artificial neural networks. However, there are several gaps between BP and learning in biologically plausible neuronal networks of the brain (learning in the brain, or simply BL, for short), in particular, (1) it has been unclear to date, if BP can be implemented exactly via BL, (2) there is a lack of local plasticity in BP, i.e., weight updates require information that is not locally available, while BL utilizes only locally available information, and (3) there is a lack of autonomy in BP, i.e., some external control over the neural network is required (e.g., switching between prediction and learning stages requires changes to dynamics and synaptic plasticity rules), while BL works fully autonomously. Bridging such gaps, i.e., understanding how BP can be approximated by BL, has been of major interest in both neuroscience and machine learning. Despite tremendous efforts, however, no previous model has bridged the gaps at a degree of demonstrating an equivalence to BP, instead, only approximations to BP have been shown. Here, we present for the first time a framework within BL that bridges the above crucial gaps. We propose a BL model that (1) produces exactly the same updates of the neural weights as BP, while (2) employing local plasticity, i.e., all neurons perform only local computations, done simultaneously. We then modify it to an alternative BL model that (3) also works fully autonomously. Overall, our work provides important evidence for the debate on the long-disputed question whether the brain can perform BP.},
  langid = {english},
  annotation = {ðŸ“ƒ}
}

@article{thieleWakesleepAlgorithmRecurrent2017,
  title = {A Wake-Sleep Algorithm for Recurrent, Spiking Neural Networks},
  author = {Thiele, Johannes and Diehl, Peter and Cook, Matthew},
  year = {2017},
  doi = {10.48550/ARXIV.1703.06290},
  urldate = {2023-04-01},
  abstract = {We investigate a recently proposed model for cortical computation which performs relational inference. It consists of several interconnected, structurally equivalent populations of leaky integrate-and-fire (LIF) neurons, which are trained in a self-organized fashion with spike-timing dependent plasticity (STDP). Despite its robust learning dynamics, the model is susceptible to a problem typical for recurrent networks which use a correlation based (Hebbian) learning rule: if trained with high learning rates, the recurrent connections can cause strong feedback loops in the network dynamics, which lead to the emergence of attractor states. This causes a strong reduction in the number of representable patterns and a decay in the inference ability of the network. As a solution, we introduce a conceptually very simple "wake-sleep" algorithm: during the wake phase, training is executed normally, while during the sleep phase, the network "dreams" samples from its generative model, which are induced by random input. This process allows us to activate the attractor states in the network, which can then be unlearned effectively by an anti-Hebbian mechanism. The algorithm allows us to increase learning rates up to a factor of ten while avoiding clustering, which allows the network to learn several times faster. Also for low learning rates, where clustering is not an issue, it improves convergence speed and reduces the final inference error.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {read},
  annotation = {âœ…}
}

@article{turrigianoHomeostaticPlasticityDeveloping2004,
  title = {Homeostatic Plasticity in the Developing Nervous System},
  author = {Turrigiano, Gina G. and Nelson, Sacha B.},
  year = {2004},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {5},
  number = {2},
  pages = {97--107},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1327},
  urldate = {2023-04-01},
  langid = {english},
  annotation = {ðŸ“ƒ}
}

@article{yamazakiSpikingNeuralNetworks2022,
  title = {Spiking {{Neural Networks}} and {{Their Applications}}: {{A Review}}},
  shorttitle = {Spiking {{Neural Networks}} and {{Their Applications}}},
  author = {Yamazaki, Kashu and {Vo-Ho}, Viet-Khoa and Bulsara, Darshan and Le, Ngan},
  year = {2022},
  month = jun,
  journal = {Brain Sciences},
  volume = {12},
  number = {7},
  pages = {863},
  issn = {2076-3425},
  doi = {10.3390/brainsci12070863},
  urldate = {2023-04-01},
  abstract = {The past decade has witnessed the great success of deep neural networks in various domains. However, deep neural networks are very resource-intensive in terms of energy consumption, data requirements, and high computational costs. With the recent increasing need for the autonomy of machines in the real world, e.g., self-driving vehicles, drones, and collaborative robots, exploitation of deep neural networks in those applications has been actively investigated. In those applications, energy and computational efficiencies are especially important because of the need for real-time responses and the limited energy supply. A promising solution to these previously infeasible applications has recently been given by biologically plausible spiking neural networks. Spiking neural networks aim to bridge the gap between neuroscience and machine learning, using biologically realistic models of neurons to carry out the computation. Due to their functional similarity to the biological neural network, spiking neural networks can embrace the sparsity found in biology and are highly compatible with temporal code. Our contributions in this work are: (i) we give a comprehensive review of theories of biological neurons; (ii) we present various existing spike-based neuron models, which have been studied in neuroscience; (iii) we detail synapse models; (iv) we provide a review of artificial neural networks; (v) we provide detailed guidance on how to train spike-based neuron models; (vi) we revise available spike-based neuron frameworks that have been developed to support implementing spiking neural networks; (vii) finally, we cover existing spiking neural network applications in computer vision and robotics domains. The paper concludes with discussions of future perspectives.},
  langid = {english},
  keywords = {read},
  annotation = {âœ…},
  file = {/home/marlon/Zotero/storage/MF79HDU6/Yamazaki et al. - 2022 - Spiking Neural Networks and Their Applications A .pdf}
}

@article{zenkeDiverseSynapticPlasticity2015,
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  year = {2015},
  month = apr,
  journal = {Nature Communications},
  volume = {6},
  number = {1},
  pages = {6922},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  urldate = {2023-04-01},
  abstract = {Abstract             Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.},
  langid = {english},
  keywords = {reading},
  annotation = {ðŸ“˜},
  file = {/home/marlon/Zotero/storage/RN27HNXU/Zenke et al. - 2015 - Diverse synaptic plasticity mechanisms orchestrate.pdf}
}

@article{zenkeDiverseSynapticPlasticity2015a,
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  year = {2015},
  month = apr,
  journal = {Nature Communications},
  volume = {6},
  number = {1},
  pages = {6922},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  urldate = {2023-04-01},
  abstract = {Abstract             Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.},
  langid = {english},
  file = {/home/marlon/Zotero/storage/CXDTBB3D/Zenke et al. - 2015 - Diverse synaptic plasticity mechanisms orchestrate.pdf}
}

@article{zenkeTemporalParadoxHebbian2017,
  title = {The Temporal Paradox of {{Hebbian}} Learning and Homeostatic Plasticity},
  author = {Zenke, Friedemann and Gerstner, Wulfram and Ganguli, Surya},
  year = {2017},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {43},
  pages = {166--176},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.03.015},
  urldate = {2023-04-01},
  langid = {english},
  annotation = {ðŸ“ƒ},
  file = {/home/marlon/Zotero/storage/QZDW6FPV/Zenke et al. - 2017 - The temporal paradox of Hebbian learning and homeo.pdf}
}
